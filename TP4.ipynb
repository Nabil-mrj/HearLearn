{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8K4TZz5WabNa"
      },
      "source": [
        "# TP 4 - Policy Gradient Methods, REINFORCE, baselines (M2AI RL  UP-SACLAY 2024-2025)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B5NXcSv_abNc"
      },
      "source": [
        "### Instructions\n",
        "This assignement will be an introduction to Policy Gradient methods. We will implement the REINFORCE algorithm and improve the variance with baselines. will combine parametrized values approximations and parametrized policy approximations.  \n",
        "\n",
        "- Save this notebook in a ```.ipynb``` format and send it to cyriaque.rousselot(at)inria(dot)fr with the name ```TP4_NAME_SURNAME``` before next sunday. Please put in object of your mail ```[RL MASTER TP4]``` followed by your name and surname.\n",
        "- Make sure to comment your code and explain your decisions clearly. Write explanations in text if necessary\n",
        "- Answers must be short and precise and don't require thousands lines of code.\n",
        "- Generally, the code to complete is indicated with the comment ```#TO IMPLEMENT```\n",
        "Good luck !\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UdoXdzHIabNd"
      },
      "source": [
        "In the previous practical session, you have seen that we can replace sampling estimations of the value by parametrized value funcitons. Here, we will look at approaches that learn a parametrized policy :\n",
        "$$ \\pi(a|s;\\theta)$$\n",
        "\n",
        "We will update this policy following gradient ascent, similarly to the previous practical session, with a different objective $J(\\theta)$ to maximize:\n",
        "\n",
        "$$ \\theta_{t+1} = \\theta_t + \\alpha \\nabla J(\\theta_t)$$\n",
        "\n",
        "$\\nabla J(\\theta_t)$ will be replaced by an approximation of the gradient of the performance of the policy.\n",
        "\n",
        "## Policy Gradient Theorem ( Reminder)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y10G-HoWabNe"
      },
      "source": [
        "We want to maximise the value from the starting state $s_0$:\n",
        "\n",
        "$$\n",
        "J(\\theta) = v_{\\pi_\\theta}(s_0).\n",
        "$$\n",
        "\n",
        "where $J(\\theta)$ represents the expected return starting from the initial state $s_0$, and $v_{\\pi_\\theta}(s)$ is the value function under the policy $\\pi_\\theta$.We have\n",
        "\n",
        "$$\n",
        "\\nabla_\\theta J(\\theta) = \\nabla_\\theta v_{\\pi_\\theta}(s_0),\n",
        "$$\n",
        "\n",
        "\n",
        "\n",
        "Expanding $v_{\\pi_\\theta}(s_0)$ in terms of the policy $\\pi_\\theta$, we get:\n",
        "\n",
        "$$\n",
        "v_{\\pi_\\theta}(s_0) = \\mathbb{E}_{\\tau \\sim \\pi_\\theta} \\left[ \\sum_{t=0}^\\infty \\gamma^t r_t \\, \\bigg| \\, s_0 \\right],\n",
        "$$\n",
        "\n",
        "where $\\tau = (s_0, a_0, s_1, a_1, \\dots)$ is a trajectory generated by the policy $\\pi_\\theta$, $\\gamma$ is the discount factor, and $r_t$ is the reward at time step $t$.\n",
        "$v_{\\pi_\\theta}(s_0)$ can be expressed in terms of the trajectory distribution induced by $\\pi_\\theta$, we have:\n",
        "\n",
        "$$\n",
        "v_{\\pi_\\theta}(s_0) = \\int_{\\tau} p_{\\pi_\\theta}(\\tau) G(\\tau),\n",
        "$$\n",
        "\n",
        "where $p_{\\pi_\\theta}(\\tau)$ is the probability of sampling a trajectory $\\tau$ under the policy $\\pi_\\theta$, and $G(\\tau) = \\sum_{t=0}^\\infty \\gamma^t r_t$ is the return of the trajectory.\n",
        "\n",
        "The trajectory probability $p_{\\pi_\\theta}(\\tau)$ can be written as:\n",
        "\n",
        "$$\n",
        "p_{\\pi_\\theta}(\\tau) = p(s_0) \\prod_{t=0}^\\infty \\pi_\\theta(a_t | s_t) p(s_{t+1} | s_t, a_t),\n",
        "$$\n",
        "\n",
        "where $p(s_0)$ is the distribution of the initial state, $\\pi_\\theta(a_t | s_t)$ is the policy, and $p(s_{t+1} | s_t, a_t)$ is the state transition probability.\n",
        "\n",
        "The gradient $\\nabla_\\theta v_{\\pi_\\theta}(s_0)$ can thus be expressed as:\n",
        "\n",
        "$$\n",
        "\\nabla_\\theta v_{\\pi_\\theta}(s_0) = \\int_{\\tau} \\nabla_\\theta p_{\\pi_\\theta}(\\tau) G(\\tau).\n",
        "$$\n",
        "\n",
        "Using the differential logarithm trick :\n",
        "\n",
        "$$\n",
        "\\nabla_\\theta p_{\\pi_\\theta}(\\tau) = p_{\\pi_\\theta}(\\tau) \\nabla_\\theta \\log p_{\\pi_\\theta}(\\tau),\n",
        "$$\n",
        "\n",
        "we obtain:\n",
        "\n",
        "$$\n",
        "\\nabla_\\theta v_{\\pi_\\theta}(s_0) = \\int_{\\tau} p_{\\pi_\\theta}(\\tau) \\nabla_\\theta \\log p_{\\pi_\\theta}(\\tau) G(\\tau).\n",
        "$$\n",
        "\n",
        "The expectation form of this integral is:\n",
        "\n",
        "$$\n",
        "\\nabla_\\theta v_{\\pi_\\theta}(s_0) = \\mathbb{E}_{\\tau \\sim \\pi_\\theta} \\left[ \\nabla_\\theta \\log p_{\\pi_\\theta}(\\tau) G(\\tau) \\right].\n",
        "$$\n",
        "\n",
        "Substituting $p_{\\pi_\\theta}(\\tau) = p(s_0) \\prod_{t=0}^\\infty \\pi_\\theta(a_t | s_t) p(s_{t+1} | s_t, a_t)$ into $\\log p_{\\pi_\\theta}(\\tau)$, we see that only $\\log \\pi_\\theta(a_t | s_t)$ depends on $\\theta$. Thus:\n",
        "\n",
        "\n",
        "$$\n",
        "\\nabla_\\theta v_{\\pi_\\theta}(s_0) = \\mathbb{E}_{\\tau \\sim \\pi_\\theta} \\left[ \\sum_{t=0}^\\infty \\nabla_\\theta \\log \\pi_\\theta(a_t | s_t) G(\\tau) \\right].\n",
        "$$\n",
        "\n",
        "Finally, since $J(\\theta) = v_{\\pi_\\theta}(s_0)$, we arrive at the policy gradient theorem:\n",
        "\n",
        "$$\n",
        "\\nabla_\\theta J(\\theta) = \\mathbb{E}_{\\tau \\sim \\pi_\\theta} \\left[ \\sum_{t=0}^\\infty \\nabla_\\theta \\log \\pi_\\theta(a_t | s_t) G(\\tau) \\right].\n",
        "$$\n",
        "\n",
        "The policy gradient theorem provides a framework for directly optimizing the policy $\\pi_\\theta$ using gradient ascent on $J(\\theta)$.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t-nwahG8abNe"
      },
      "source": [
        "## Policy network design"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4o27HHXVabNf"
      },
      "source": [
        "We will decompose in two ways the building of the policy :\n",
        "+ First, we try a simple linear model $\\pi^{softmax}_\\theta(a| s)$\n",
        "+ Second we try a more complex neural network $\\pi^{network}_\\theta(a| s)$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G30Hk8-PabNf"
      },
      "source": [
        "> If you don't succeed to build $\\pi^{softmax}_\\theta(a| s)$, you can still implement the REINFORCE algorithm for the following $\\pi^{network}_\\theta(a| s)$  ( see next part)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TdskvKLdabNg"
      },
      "source": [
        "### Softmax method\n",
        "\n",
        "If the action space is discrete and not too large, then a natural and common kind of\n",
        "parameterization is to form parameterized numerical preferences $h(s, a, \\theta) \\in R $ for each\n",
        "state–action pair."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H9pvHVdCabNh"
      },
      "source": [
        "Let's build $h(s, a, \\theta)$ using feature representations:\n",
        "   $$\n",
        "   \\phi(s, a)^\\top \\theta = h(s, a, \\theta)\n",
        "   $$\n",
        "and $\\pi^{softmax}_\\theta(a| s)$:\n",
        "$$\n",
        "\\pi^{softmax}_\\theta(a| s) = \\frac{e^{h(s,a,\\theta)}}{\\sum_b e^{h(s,b,\\theta)}}\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zlCJHVSCabNh"
      },
      "source": [
        "The actions with the highest preferences in each state are given the\n",
        "highest probabilities of being selected."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QI9R67DOabNh"
      },
      "source": [
        "Let's build our $\\phi$."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install gymnasium"
      ],
      "metadata": {
        "id": "QG9o9nL4b1HJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "51bb8d5c-5a3c-4e83-c6d0-381419a8fdf7"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gymnasium\n",
            "  Downloading gymnasium-1.0.0-py3-none-any.whl.metadata (9.5 kB)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (1.26.4)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (4.12.2)\n",
            "Collecting farama-notifications>=0.0.1 (from gymnasium)\n",
            "  Downloading Farama_Notifications-0.0.4-py3-none-any.whl.metadata (558 bytes)\n",
            "Downloading gymnasium-1.0.0-py3-none-any.whl (958 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m958.1/958.1 kB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading Farama_Notifications-0.0.4-py3-none-any.whl (2.5 kB)\n",
            "Installing collected packages: farama-notifications, gymnasium\n",
            "Successfully installed farama-notifications-0.0.4 gymnasium-1.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "DTFT9_LQabNh"
      },
      "outputs": [],
      "source": [
        "from sklearn.kernel_approximation import RBFSampler\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "import numpy as np\n",
        "import gymnasium as gym\n",
        "# train phi for the mountain car environment :\n",
        "\n",
        "n_components = 5\n",
        "preprocessing_states = Pipeline(\n",
        "    [\n",
        "        (\"scaler\", StandardScaler()),\n",
        "        (\"feature_generation\", RBFSampler(n_components=n_components)),\n",
        "    ]\n",
        ")\n",
        "\n",
        "preprocessing_states.fit(\n",
        "    np.array(\n",
        "        [\n",
        "            [\n",
        "                np.random.uniform(-4.8, 4.8),\n",
        "                np.random.normal(0, 1),\n",
        "                np.random.uniform(-0.418, 0.418),\n",
        "                np.random.normal(0, 1),\n",
        "                np.random.choice([0, 1, 2]),\n",
        "            ]\n",
        "            for _ in range(10000)\n",
        "        ]\n",
        "    )\n",
        ")\n",
        "\n",
        "\n",
        "def phi(s, a) -> np.ndarray:\n",
        "    action = np.array(a)\n",
        "    state = np.array(s)\n",
        "    return preprocessing_states.transform([np.hstack([state, action])])[0]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S1J4ihY9abNi"
      },
      "source": [
        "Let's use CartPole for now to test our $\\phi$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mKZBLDXxabNi",
        "outputId": "36335c5e-3b87-4873-f1b6-e6c4d9be9d74"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Example of phi(s,a)  [-0.61531422  0.33184415  0.43636739 -0.0449666   0.59365549]\n"
          ]
        }
      ],
      "source": [
        "# instantiate the environment\n",
        "env = gym.make(\"CartPole-v1\", render_mode=\"rgb_array\")\n",
        "state, _ = env.reset()\n",
        "action = env.action_space.sample()\n",
        "\n",
        "print(\"Example of phi(s,a) \", phi(state, action))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IzJ1ondtabNi"
      },
      "source": [
        "Let's define our h :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "Wz4rXDOcabNi"
      },
      "outputs": [],
      "source": [
        "def h(a, state, theta):\n",
        "    return np.dot(phi(state, a), theta)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "msATWh-6abNj"
      },
      "source": [
        "**Q.1 : Using the previous $\\phi$, implement a  function softmax_policy that return an action selected from the  distribution $\\pi^{softmax}_\\theta(a| s)$ for the CartPole environment and the distribution $\\pi^{softmax}_\\theta(a| s)$**\n",
        "$$\n",
        "\\pi^{softmax}_\\theta(a| s) = \\frac{e^{h(s,a,\\theta)}}{\\sum_b e^{h(s,b,\\theta)}}\n",
        "$$\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "h4MDLNlgabNk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cec01843-f832-4306-98e3-fbf394816d7a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1, array([0.48692496, 0.51307504]))"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ],
      "source": [
        "theta = np.random.normal(0, 1, size=(n_components))\n",
        "\n",
        "\n",
        "def softmax_policy(state, h, theta=theta):\n",
        "    # Compute preferences for both actions (0 and 1 in CartPole)\n",
        "    actions = [0, 1]\n",
        "    preferences = np.array([h(a, state, theta) for a in actions])\n",
        "\n",
        "    # Apply softmax to compute probabilities\n",
        "    exp_preferences = np.exp(preferences - np.max(preferences))  # Numerical stability\n",
        "    current_choice_sample = current_choice_sample = exp_preferences / np.sum(exp_preferences)\n",
        "    return np.random.choice([0, 1], p=current_choice_sample), current_choice_sample\n",
        "\n",
        "\n",
        "softmax_policy(state, h)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lkuRFIx9abNk"
      },
      "source": [
        "**Q.2 Show that  $\\nabla_\\theta  \\log \\pi^{softmax}\\theta(a| s) = \\phi(s,a) - \\sum_b \\pi^{softmax}_\\theta(b| s) \\phi(s,b)$**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\\begin{align*}\n",
        "\\nabla_\\theta \\log \\pi^{softmax}_\\theta(a| s) &= \\nabla_\\theta \\log \\left( \\frac{e^{\\phi(s,a)^\\top \\theta}}{\\sum_b e^{\\phi(s,b)^\\top \\theta}} \\right) \\\\\n",
        "&= \\nabla_\\theta \\left( \\phi(s,a)^\\top \\theta - \\log \\left( \\sum_b e^{\\phi(s,b)^\\top \\theta} \\right) \\right) \\\\\n",
        "&= \\nabla_\\theta \\phi(s,a)^\\top \\theta - \\nabla_\\theta \\log \\left( \\sum_b e^{\\phi(s,b)^\\top \\theta} \\right) \\\\\n",
        "&= \\phi(s,a) - \\nabla_\\theta \\log \\left( \\sum_b e^{\\phi(s,b)^\\top \\theta} \\right).\n",
        "\\end{align*}\n",
        "\n",
        "For the second term:\n",
        "\\begin{align*}\n",
        "\\nabla_\\theta \\log \\left( \\sum_b e^{\\phi(s,b)^\\top \\theta} \\right) &= \\frac{\\nabla_\\theta \\sum_b e^{\\phi(s,b)^\\top \\theta}}{\\sum_b e^{\\phi(s,b)^\\top \\theta}} \\\\\n",
        "&= \\frac{\\sum_b e^{\\phi(s,b)^\\top \\theta} \\nabla_\\theta \\phi(s,b)^\\top \\theta}{\\sum_b e^{\\phi(s,b)^\\top \\theta}} \\\\\n",
        "&= \\sum_b \\pi^{softmax}_\\theta(b| s) \\phi(s,b).\n",
        "\\end{align*}\n",
        "\n",
        "Finally:\n",
        "\\begin{align*}\n",
        "\\nabla_\\theta \\log \\pi^{softmax}_\\theta(a| s) &= \\phi(s,a) - \\sum_b \\pi^{softmax}_\\theta(b| s) \\phi(s,b).\n",
        "\\end{align*}"
      ],
      "metadata": {
        "id": "nqIKOxt4ktRs"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HxHIFl4LabNk"
      },
      "source": [
        "**Q.3 Implement a function that return $\\nabla_\\theta  \\log \\pi^{softmax}\\theta(a| s)$**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "N3994a1iabNk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "05c82443-62f1-4e18-e86b-42446906e63b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gradient [-0.45392287  0.63473823  0.6478408   0.28762471  0.00831014]\n"
          ]
        }
      ],
      "source": [
        "def grad_softmax(s, a):\n",
        "    choice, pi = softmax_policy(s, h, theta)\n",
        "    phi_a = phi(s, a)\n",
        "    phi_sum = sum(pi[b] * phi(s, b) for b in range(len(pi)))\n",
        "    return phi_a - phi_sum\n",
        "\n",
        "\n",
        "print(\"Gradient\", grad_softmax(state, action))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d3qfF9wQabNl"
      },
      "source": [
        "## Neural Network policy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VgMUahNyabNl"
      },
      "source": [
        "Definition of the network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "grRVMPTuabNl"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class PolicyNetwork(nn.Module):\n",
        "    def __init__(self, state_dim, action_dim, hidden_dim=128):\n",
        "        super(PolicyNetwork, self).__init__()\n",
        "        # Define a simple feedforward network\n",
        "        self.fc1 = nn.Linear(state_dim, hidden_dim)  # First fully connected layer\n",
        "        self.fc2 = nn.Linear(hidden_dim, action_dim)  # Output layer for action logits\n",
        "\n",
        "    def forward(self, state):\n",
        "        \"\"\"\n",
        "        Forward pass to compute action probabilities.\n",
        "        Args:\n",
        "            state (torch.Tensor): The input state tensor of shape (batch_size, state_dim).\n",
        "        Returns:\n",
        "            torch.Tensor: Action probabilities of shape (batch_size, action_dim).\n",
        "        \"\"\"\n",
        "        x = F.relu(self.fc1(state))  # Apply ReLU activation to the first layer\n",
        "        logits = self.fc2(x)  # Compute action logits\n",
        "        action_probs = F.softmax(logits, dim=-1)  # Convert logits to probabilities\n",
        "        return action_probs\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R_BOZnwuabNl"
      },
      "source": [
        "Apply the policy network to the environment to get probabilities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "tq1pzEvsabNl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ecfef057-8e8c-4991-a9fc-0521b519b686"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Action probabilities: tensor([[0.5310, 0.4690]], grad_fn=<SoftmaxBackward0>)\n",
            "Action chosen 0\n"
          ]
        }
      ],
      "source": [
        "import gymnasium as gym\n",
        "import torch\n",
        "\n",
        "# Create CartPole environment\n",
        "env = gym.make(\"CartPole-v1\")\n",
        "\n",
        "# Initialize the policy network\n",
        "state_dim = env.observation_space.shape[0]  # Dimension of the state space\n",
        "action_dim = env.action_space.n  # Number of discrete actions\n",
        "policy_net = PolicyNetwork(state_dim, action_dim)\n",
        "\n",
        "# Example forward pass\n",
        "state, _ = env.reset()  # Reset the environment to get the initial state\n",
        "torch_state = torch.FloatTensor(state).unsqueeze(0)  # Convert state to tensor with batch dimension\n",
        "action_probs = policy_net(torch_state)\n",
        "\n",
        "# Print action probabilities and choose an action using torch.distributions.Categorical\n",
        "print(\"Action probabilities:\", action_probs)\n",
        "action_distribution = torch.distributions.Categorical(action_probs)\n",
        "action = action_distribution.sample()\n",
        "print(\"Action chosen\", action.item())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MSjr5nJ6abNm"
      },
      "source": [
        "## REINFORCE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S2I0sIAGabNm"
      },
      "source": [
        "The pronciple of the REINFORCE algorithm is this following :"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2DFRQhmgabNm"
      },
      "source": [
        "+ Collect trajectories using the current policy $\\pi_\\theta$.\n",
        "+ Compute the returns $G_t = \\sum_{t'} \\gamma^{t'-t} r_{t'}$.  \n",
        "+ Apply the policy gradient update:\n",
        "\n",
        "$$\n",
        "\\nabla_\\theta J(\\theta) = \\mathbb{E}_{\\tau \\sim \\pi_\\theta} \\left[ \\nabla_\\theta \\log \\pi_\\theta(a_t | s_t) G(\\tau) \\right],\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\theta_{t+1} = \\theta_{t} + \\alpha G_t  \\nabla_\\theta  \\log \\pi_\\theta(a_t | s_t)\n",
        "$$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3FCQXPjCabNm"
      },
      "source": [
        " ![reinforce_algorithm](REINGORCE.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-5LAKo4KabNm"
      },
      "source": [
        "**Q.4 Build a function train_reinforce to solve the environment CartPole using the $\\pi^{softmax}_\\theta(a| s)$ policy updated using REINFORCE**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "GsxdJ4kmabNn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2f55254c-7136-4911-f8d8-ad4df8205468"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 1, Total Reward: 84.0\n",
            "Episode 2, Total Reward: 46.0\n",
            "Episode 3, Total Reward: 80.0\n",
            "Episode 4, Total Reward: 77.0\n",
            "Episode 5, Total Reward: 60.0\n",
            "Episode 6, Total Reward: 20.0\n",
            "Episode 7, Total Reward: 54.0\n",
            "Episode 8, Total Reward: 60.0\n",
            "Episode 9, Total Reward: 98.0\n",
            "Episode 10, Total Reward: 27.0\n",
            "Episode 11, Total Reward: 94.0\n",
            "Episode 12, Total Reward: 93.0\n",
            "Episode 13, Total Reward: 70.0\n",
            "Episode 14, Total Reward: 75.0\n",
            "Episode 15, Total Reward: 26.0\n",
            "Episode 16, Total Reward: 70.0\n",
            "Episode 17, Total Reward: 78.0\n",
            "Episode 18, Total Reward: 145.0\n",
            "Episode 19, Total Reward: 99.0\n",
            "Episode 20, Total Reward: 92.0\n",
            "Episode 21, Total Reward: 55.0\n",
            "Episode 22, Total Reward: 29.0\n",
            "Episode 23, Total Reward: 123.0\n",
            "Episode 24, Total Reward: 118.0\n",
            "Episode 25, Total Reward: 74.0\n",
            "Episode 26, Total Reward: 47.0\n",
            "Episode 27, Total Reward: 59.0\n",
            "Episode 28, Total Reward: 83.0\n",
            "Episode 29, Total Reward: 80.0\n",
            "Episode 30, Total Reward: 92.0\n",
            "Episode 31, Total Reward: 84.0\n",
            "Episode 32, Total Reward: 126.0\n",
            "Episode 33, Total Reward: 56.0\n",
            "Episode 34, Total Reward: 77.0\n",
            "Episode 35, Total Reward: 142.0\n",
            "Episode 36, Total Reward: 76.0\n",
            "Episode 37, Total Reward: 87.0\n",
            "Episode 38, Total Reward: 64.0\n",
            "Episode 39, Total Reward: 90.0\n",
            "Episode 40, Total Reward: 60.0\n",
            "Episode 41, Total Reward: 66.0\n",
            "Episode 42, Total Reward: 92.0\n",
            "Episode 43, Total Reward: 101.0\n",
            "Episode 44, Total Reward: 34.0\n",
            "Episode 45, Total Reward: 181.0\n",
            "Episode 46, Total Reward: 104.0\n",
            "Episode 47, Total Reward: 84.0\n",
            "Episode 48, Total Reward: 61.0\n",
            "Episode 49, Total Reward: 154.0\n",
            "Episode 50, Total Reward: 34.0\n",
            "Episode 51, Total Reward: 46.0\n",
            "Episode 52, Total Reward: 26.0\n",
            "Episode 53, Total Reward: 20.0\n",
            "Episode 54, Total Reward: 45.0\n",
            "Episode 55, Total Reward: 63.0\n",
            "Episode 56, Total Reward: 75.0\n",
            "Episode 57, Total Reward: 116.0\n",
            "Episode 58, Total Reward: 63.0\n",
            "Episode 59, Total Reward: 82.0\n",
            "Episode 60, Total Reward: 9.0\n",
            "Episode 61, Total Reward: 73.0\n",
            "Episode 62, Total Reward: 58.0\n",
            "Episode 63, Total Reward: 83.0\n",
            "Episode 64, Total Reward: 76.0\n",
            "Episode 65, Total Reward: 70.0\n",
            "Episode 66, Total Reward: 73.0\n",
            "Episode 67, Total Reward: 99.0\n",
            "Episode 68, Total Reward: 88.0\n",
            "Episode 69, Total Reward: 88.0\n",
            "Episode 70, Total Reward: 71.0\n",
            "Episode 71, Total Reward: 63.0\n",
            "Episode 72, Total Reward: 56.0\n",
            "Episode 73, Total Reward: 68.0\n",
            "Episode 74, Total Reward: 33.0\n",
            "Episode 75, Total Reward: 57.0\n",
            "Episode 76, Total Reward: 65.0\n",
            "Episode 77, Total Reward: 102.0\n",
            "Episode 78, Total Reward: 46.0\n",
            "Episode 79, Total Reward: 129.0\n",
            "Episode 80, Total Reward: 92.0\n",
            "Episode 81, Total Reward: 76.0\n",
            "Episode 82, Total Reward: 98.0\n",
            "Episode 83, Total Reward: 53.0\n",
            "Episode 84, Total Reward: 106.0\n",
            "Episode 85, Total Reward: 61.0\n",
            "Episode 86, Total Reward: 111.0\n",
            "Episode 87, Total Reward: 111.0\n",
            "Episode 88, Total Reward: 44.0\n",
            "Episode 89, Total Reward: 45.0\n",
            "Episode 90, Total Reward: 45.0\n",
            "Episode 91, Total Reward: 62.0\n",
            "Episode 92, Total Reward: 54.0\n",
            "Episode 93, Total Reward: 58.0\n",
            "Episode 94, Total Reward: 66.0\n",
            "Episode 95, Total Reward: 58.0\n",
            "Episode 96, Total Reward: 97.0\n",
            "Episode 97, Total Reward: 142.0\n",
            "Episode 98, Total Reward: 107.0\n",
            "Episode 99, Total Reward: 87.0\n",
            "Episode 100, Total Reward: 173.0\n",
            "Episode 101, Total Reward: 127.0\n",
            "Episode 102, Total Reward: 29.0\n",
            "Episode 103, Total Reward: 89.0\n",
            "Episode 104, Total Reward: 96.0\n",
            "Episode 105, Total Reward: 58.0\n",
            "Episode 106, Total Reward: 35.0\n",
            "Episode 107, Total Reward: 60.0\n",
            "Episode 108, Total Reward: 109.0\n",
            "Episode 109, Total Reward: 85.0\n",
            "Episode 110, Total Reward: 97.0\n",
            "Episode 111, Total Reward: 86.0\n",
            "Episode 112, Total Reward: 43.0\n",
            "Episode 113, Total Reward: 72.0\n",
            "Episode 114, Total Reward: 60.0\n",
            "Episode 115, Total Reward: 63.0\n",
            "Episode 116, Total Reward: 75.0\n",
            "Episode 117, Total Reward: 84.0\n",
            "Episode 118, Total Reward: 78.0\n",
            "Episode 119, Total Reward: 48.0\n",
            "Episode 120, Total Reward: 78.0\n",
            "Episode 121, Total Reward: 144.0\n",
            "Episode 122, Total Reward: 164.0\n",
            "Episode 123, Total Reward: 50.0\n",
            "Episode 124, Total Reward: 87.0\n",
            "Episode 125, Total Reward: 65.0\n",
            "Episode 126, Total Reward: 60.0\n",
            "Episode 127, Total Reward: 69.0\n",
            "Episode 128, Total Reward: 100.0\n",
            "Episode 129, Total Reward: 45.0\n",
            "Episode 130, Total Reward: 49.0\n",
            "Episode 131, Total Reward: 18.0\n",
            "Episode 132, Total Reward: 103.0\n",
            "Episode 133, Total Reward: 32.0\n",
            "Episode 134, Total Reward: 48.0\n",
            "Episode 135, Total Reward: 24.0\n",
            "Episode 136, Total Reward: 64.0\n",
            "Episode 137, Total Reward: 74.0\n",
            "Episode 138, Total Reward: 93.0\n",
            "Episode 139, Total Reward: 67.0\n",
            "Episode 140, Total Reward: 188.0\n",
            "Episode 141, Total Reward: 70.0\n",
            "Episode 142, Total Reward: 42.0\n",
            "Episode 143, Total Reward: 66.0\n",
            "Episode 144, Total Reward: 65.0\n",
            "Episode 145, Total Reward: 84.0\n",
            "Episode 146, Total Reward: 77.0\n",
            "Episode 147, Total Reward: 50.0\n",
            "Episode 148, Total Reward: 108.0\n",
            "Episode 149, Total Reward: 68.0\n",
            "Episode 150, Total Reward: 92.0\n",
            "Episode 151, Total Reward: 161.0\n",
            "Episode 152, Total Reward: 62.0\n",
            "Episode 153, Total Reward: 116.0\n",
            "Episode 154, Total Reward: 84.0\n",
            "Episode 155, Total Reward: 48.0\n",
            "Episode 156, Total Reward: 71.0\n",
            "Episode 157, Total Reward: 56.0\n",
            "Episode 158, Total Reward: 33.0\n",
            "Episode 159, Total Reward: 60.0\n",
            "Episode 160, Total Reward: 57.0\n",
            "Episode 161, Total Reward: 56.0\n",
            "Episode 162, Total Reward: 15.0\n",
            "Episode 163, Total Reward: 46.0\n",
            "Episode 164, Total Reward: 63.0\n",
            "Episode 165, Total Reward: 54.0\n",
            "Episode 166, Total Reward: 86.0\n",
            "Episode 167, Total Reward: 90.0\n",
            "Episode 168, Total Reward: 97.0\n",
            "Episode 169, Total Reward: 61.0\n",
            "Episode 170, Total Reward: 36.0\n",
            "Episode 171, Total Reward: 49.0\n",
            "Episode 172, Total Reward: 17.0\n",
            "Episode 173, Total Reward: 43.0\n",
            "Episode 174, Total Reward: 140.0\n",
            "Episode 175, Total Reward: 113.0\n",
            "Episode 176, Total Reward: 84.0\n",
            "Episode 177, Total Reward: 34.0\n",
            "Episode 178, Total Reward: 87.0\n",
            "Episode 179, Total Reward: 51.0\n",
            "Episode 180, Total Reward: 56.0\n",
            "Episode 181, Total Reward: 117.0\n",
            "Episode 182, Total Reward: 69.0\n",
            "Episode 183, Total Reward: 83.0\n",
            "Episode 184, Total Reward: 43.0\n",
            "Episode 185, Total Reward: 63.0\n",
            "Episode 186, Total Reward: 75.0\n",
            "Episode 187, Total Reward: 57.0\n",
            "Episode 188, Total Reward: 100.0\n",
            "Episode 189, Total Reward: 55.0\n",
            "Episode 190, Total Reward: 60.0\n",
            "Episode 191, Total Reward: 80.0\n",
            "Episode 192, Total Reward: 61.0\n",
            "Episode 193, Total Reward: 52.0\n",
            "Episode 194, Total Reward: 19.0\n",
            "Episode 195, Total Reward: 54.0\n",
            "Episode 196, Total Reward: 51.0\n",
            "Episode 197, Total Reward: 103.0\n",
            "Episode 198, Total Reward: 64.0\n",
            "Episode 199, Total Reward: 78.0\n",
            "Episode 200, Total Reward: 52.0\n",
            "Episode 201, Total Reward: 50.0\n",
            "Episode 202, Total Reward: 105.0\n",
            "Episode 203, Total Reward: 46.0\n",
            "Episode 204, Total Reward: 110.0\n",
            "Episode 205, Total Reward: 90.0\n",
            "Episode 206, Total Reward: 38.0\n",
            "Episode 207, Total Reward: 62.0\n",
            "Episode 208, Total Reward: 188.0\n",
            "Episode 209, Total Reward: 56.0\n",
            "Episode 210, Total Reward: 48.0\n",
            "Episode 211, Total Reward: 63.0\n",
            "Episode 212, Total Reward: 66.0\n",
            "Episode 213, Total Reward: 109.0\n",
            "Episode 214, Total Reward: 57.0\n",
            "Episode 215, Total Reward: 146.0\n",
            "Episode 216, Total Reward: 82.0\n",
            "Episode 217, Total Reward: 110.0\n",
            "Episode 218, Total Reward: 137.0\n",
            "Episode 219, Total Reward: 38.0\n",
            "Episode 220, Total Reward: 34.0\n",
            "Episode 221, Total Reward: 108.0\n",
            "Episode 222, Total Reward: 153.0\n",
            "Episode 223, Total Reward: 45.0\n",
            "Episode 224, Total Reward: 62.0\n",
            "Episode 225, Total Reward: 57.0\n",
            "Episode 226, Total Reward: 73.0\n",
            "Episode 227, Total Reward: 139.0\n",
            "Episode 228, Total Reward: 22.0\n",
            "Episode 229, Total Reward: 70.0\n",
            "Episode 230, Total Reward: 23.0\n",
            "Episode 231, Total Reward: 32.0\n",
            "Episode 232, Total Reward: 140.0\n",
            "Episode 233, Total Reward: 18.0\n",
            "Episode 234, Total Reward: 54.0\n",
            "Episode 235, Total Reward: 62.0\n",
            "Episode 236, Total Reward: 105.0\n",
            "Episode 237, Total Reward: 57.0\n",
            "Episode 238, Total Reward: 143.0\n",
            "Episode 239, Total Reward: 72.0\n",
            "Episode 240, Total Reward: 82.0\n",
            "Episode 241, Total Reward: 132.0\n",
            "Episode 242, Total Reward: 60.0\n",
            "Episode 243, Total Reward: 45.0\n",
            "Episode 244, Total Reward: 71.0\n",
            "Episode 245, Total Reward: 81.0\n",
            "Episode 246, Total Reward: 108.0\n",
            "Episode 247, Total Reward: 156.0\n",
            "Episode 248, Total Reward: 77.0\n",
            "Episode 249, Total Reward: 83.0\n",
            "Episode 250, Total Reward: 40.0\n",
            "Episode 251, Total Reward: 96.0\n",
            "Episode 252, Total Reward: 86.0\n",
            "Episode 253, Total Reward: 60.0\n",
            "Episode 254, Total Reward: 72.0\n",
            "Episode 255, Total Reward: 66.0\n",
            "Episode 256, Total Reward: 36.0\n",
            "Episode 257, Total Reward: 81.0\n",
            "Episode 258, Total Reward: 81.0\n",
            "Episode 259, Total Reward: 71.0\n",
            "Episode 260, Total Reward: 33.0\n",
            "Episode 261, Total Reward: 133.0\n",
            "Episode 262, Total Reward: 91.0\n",
            "Episode 263, Total Reward: 51.0\n",
            "Episode 264, Total Reward: 75.0\n",
            "Episode 265, Total Reward: 77.0\n",
            "Episode 266, Total Reward: 63.0\n",
            "Episode 267, Total Reward: 86.0\n",
            "Episode 268, Total Reward: 65.0\n",
            "Episode 269, Total Reward: 88.0\n",
            "Episode 270, Total Reward: 57.0\n",
            "Episode 271, Total Reward: 99.0\n",
            "Episode 272, Total Reward: 54.0\n",
            "Episode 273, Total Reward: 92.0\n",
            "Episode 274, Total Reward: 71.0\n",
            "Episode 275, Total Reward: 44.0\n",
            "Episode 276, Total Reward: 41.0\n",
            "Episode 277, Total Reward: 157.0\n",
            "Episode 278, Total Reward: 93.0\n",
            "Episode 279, Total Reward: 59.0\n",
            "Episode 280, Total Reward: 90.0\n",
            "Episode 281, Total Reward: 230.0\n",
            "Environment solved!\n"
          ]
        }
      ],
      "source": [
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "# Function to compute discounted returns\n",
        "def compute_returns(rewards, gamma=0.99):\n",
        "    discounted_returns = []\n",
        "    G = 0\n",
        "    for reward in reversed(rewards):\n",
        "        G = reward + gamma * G\n",
        "        discounted_returns.insert(0, G)\n",
        "    return discounted_returns\n",
        "\n",
        "def train_reinforce(env_name=\"CartPole-v1\", episodes=1000, gamma=0.99, lr=1e-3):\n",
        "    # Initialize environment and parameters of the softmax policy\n",
        "    env = gym.make(env_name)\n",
        "    theta = np.random.normal(0, 1, size=(n_components))\n",
        "    for episode in range(episodes):\n",
        "        state, _ = env.reset()\n",
        "\n",
        "        rewards = []\n",
        "        states = []\n",
        "        actions = []\n",
        "\n",
        "\n",
        "        # Collect a single trajectory\n",
        "        done = False\n",
        "        while not done:\n",
        "            action,probs = softmax_policy(state, h, theta) # TODO : Select an action using the softmax policy\n",
        "            next_state, reward, done, _, _ = env.step(action)\n",
        "            rewards.append(reward)\n",
        "            states.append(state)\n",
        "            actions.append(action)\n",
        "            state = next_state\n",
        "\n",
        "        # Compute discounted returns\n",
        "        returns = compute_returns(rewards, gamma)\n",
        "        returns = np.array(returns)\n",
        "\n",
        "        # Normalize returns for numerical stability\n",
        "        returns = (returns - returns.mean()) / (returns.std() + 1e-9)\n",
        "\n",
        "        # TODO Compute policy gradient loss\n",
        "        policy_gradient_loss = np.zeros_like(theta)\n",
        "        for t, Gt in enumerate(returns):\n",
        "          policy_gradient_loss += Gt * grad_softmax(states[t], actions[t])\n",
        "\n",
        "        # TODO Backward pass: Update the softmax policy weights\n",
        "        theta += lr * policy_gradient_loss\n",
        "\n",
        "\n",
        "        # Logging\n",
        "        total_reward = sum(rewards)\n",
        "        print(f\"Episode {episode + 1}, Total Reward: {total_reward}\")\n",
        "\n",
        "        # Optional: Stop training early if the task is solved\n",
        "        if total_reward >= 195:  # CartPole-v1 is considered solved at 195\n",
        "            print(\"Environment solved!\")\n",
        "            break\n",
        "\n",
        "    env.close()\n",
        "    return theta\n",
        "\n",
        "\n",
        "# Train the policy weights using REINFORCE\n",
        "trained_policy = train_reinforce()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fw6M6W3sabNn"
      },
      "source": [
        "**Q.5 Build a function train_reinforce to solve the environment CartPole using the $\\pi^{network}_\\theta(a| s)$ policy updated using REINFORCE**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "LG1GyuzkabNn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "53245b7e-741b-4c1d-f881-1e4ddf3f509c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 1, Total Reward: 23.0\n",
            "Episode 2, Total Reward: 75.0\n",
            "Episode 3, Total Reward: 19.0\n",
            "Episode 4, Total Reward: 14.0\n",
            "Episode 5, Total Reward: 62.0\n",
            "Episode 6, Total Reward: 98.0\n",
            "Episode 7, Total Reward: 27.0\n",
            "Episode 8, Total Reward: 44.0\n",
            "Episode 9, Total Reward: 14.0\n",
            "Episode 10, Total Reward: 27.0\n",
            "Episode 11, Total Reward: 24.0\n",
            "Episode 12, Total Reward: 29.0\n",
            "Episode 13, Total Reward: 40.0\n",
            "Episode 14, Total Reward: 59.0\n",
            "Episode 15, Total Reward: 18.0\n",
            "Episode 16, Total Reward: 22.0\n",
            "Episode 17, Total Reward: 20.0\n",
            "Episode 18, Total Reward: 33.0\n",
            "Episode 19, Total Reward: 13.0\n",
            "Episode 20, Total Reward: 43.0\n",
            "Episode 21, Total Reward: 17.0\n",
            "Episode 22, Total Reward: 22.0\n",
            "Episode 23, Total Reward: 41.0\n",
            "Episode 24, Total Reward: 41.0\n",
            "Episode 25, Total Reward: 13.0\n",
            "Episode 26, Total Reward: 15.0\n",
            "Episode 27, Total Reward: 15.0\n",
            "Episode 28, Total Reward: 20.0\n",
            "Episode 29, Total Reward: 36.0\n",
            "Episode 30, Total Reward: 16.0\n",
            "Episode 31, Total Reward: 15.0\n",
            "Episode 32, Total Reward: 21.0\n",
            "Episode 33, Total Reward: 22.0\n",
            "Episode 34, Total Reward: 39.0\n",
            "Episode 35, Total Reward: 32.0\n",
            "Episode 36, Total Reward: 18.0\n",
            "Episode 37, Total Reward: 18.0\n",
            "Episode 38, Total Reward: 19.0\n",
            "Episode 39, Total Reward: 14.0\n",
            "Episode 40, Total Reward: 19.0\n",
            "Episode 41, Total Reward: 50.0\n",
            "Episode 42, Total Reward: 17.0\n",
            "Episode 43, Total Reward: 16.0\n",
            "Episode 44, Total Reward: 56.0\n",
            "Episode 45, Total Reward: 23.0\n",
            "Episode 46, Total Reward: 24.0\n",
            "Episode 47, Total Reward: 39.0\n",
            "Episode 48, Total Reward: 23.0\n",
            "Episode 49, Total Reward: 9.0\n",
            "Episode 50, Total Reward: 80.0\n",
            "Episode 51, Total Reward: 121.0\n",
            "Episode 52, Total Reward: 37.0\n",
            "Episode 53, Total Reward: 35.0\n",
            "Episode 54, Total Reward: 22.0\n",
            "Episode 55, Total Reward: 12.0\n",
            "Episode 56, Total Reward: 11.0\n",
            "Episode 57, Total Reward: 29.0\n",
            "Episode 58, Total Reward: 31.0\n",
            "Episode 59, Total Reward: 64.0\n",
            "Episode 60, Total Reward: 32.0\n",
            "Episode 61, Total Reward: 20.0\n",
            "Episode 62, Total Reward: 46.0\n",
            "Episode 63, Total Reward: 35.0\n",
            "Episode 64, Total Reward: 32.0\n",
            "Episode 65, Total Reward: 39.0\n",
            "Episode 66, Total Reward: 21.0\n",
            "Episode 67, Total Reward: 17.0\n",
            "Episode 68, Total Reward: 20.0\n",
            "Episode 69, Total Reward: 124.0\n",
            "Episode 70, Total Reward: 19.0\n",
            "Episode 71, Total Reward: 16.0\n",
            "Episode 72, Total Reward: 40.0\n",
            "Episode 73, Total Reward: 29.0\n",
            "Episode 74, Total Reward: 47.0\n",
            "Episode 75, Total Reward: 59.0\n",
            "Episode 76, Total Reward: 60.0\n",
            "Episode 77, Total Reward: 28.0\n",
            "Episode 78, Total Reward: 39.0\n",
            "Episode 79, Total Reward: 29.0\n",
            "Episode 80, Total Reward: 25.0\n",
            "Episode 81, Total Reward: 71.0\n",
            "Episode 82, Total Reward: 38.0\n",
            "Episode 83, Total Reward: 49.0\n",
            "Episode 84, Total Reward: 19.0\n",
            "Episode 85, Total Reward: 41.0\n",
            "Episode 86, Total Reward: 21.0\n",
            "Episode 87, Total Reward: 24.0\n",
            "Episode 88, Total Reward: 21.0\n",
            "Episode 89, Total Reward: 39.0\n",
            "Episode 90, Total Reward: 53.0\n",
            "Episode 91, Total Reward: 70.0\n",
            "Episode 92, Total Reward: 17.0\n",
            "Episode 93, Total Reward: 47.0\n",
            "Episode 94, Total Reward: 17.0\n",
            "Episode 95, Total Reward: 24.0\n",
            "Episode 96, Total Reward: 19.0\n",
            "Episode 97, Total Reward: 92.0\n",
            "Episode 98, Total Reward: 61.0\n",
            "Episode 99, Total Reward: 39.0\n",
            "Episode 100, Total Reward: 20.0\n",
            "Episode 101, Total Reward: 86.0\n",
            "Episode 102, Total Reward: 67.0\n",
            "Episode 103, Total Reward: 51.0\n",
            "Episode 104, Total Reward: 52.0\n",
            "Episode 105, Total Reward: 29.0\n",
            "Episode 106, Total Reward: 24.0\n",
            "Episode 107, Total Reward: 46.0\n",
            "Episode 108, Total Reward: 26.0\n",
            "Episode 109, Total Reward: 84.0\n",
            "Episode 110, Total Reward: 95.0\n",
            "Episode 111, Total Reward: 48.0\n",
            "Episode 112, Total Reward: 22.0\n",
            "Episode 113, Total Reward: 18.0\n",
            "Episode 114, Total Reward: 52.0\n",
            "Episode 115, Total Reward: 73.0\n",
            "Episode 116, Total Reward: 43.0\n",
            "Episode 117, Total Reward: 60.0\n",
            "Episode 118, Total Reward: 60.0\n",
            "Episode 119, Total Reward: 14.0\n",
            "Episode 120, Total Reward: 29.0\n",
            "Episode 121, Total Reward: 122.0\n",
            "Episode 122, Total Reward: 30.0\n",
            "Episode 123, Total Reward: 50.0\n",
            "Episode 124, Total Reward: 38.0\n",
            "Episode 125, Total Reward: 94.0\n",
            "Episode 126, Total Reward: 167.0\n",
            "Episode 127, Total Reward: 18.0\n",
            "Episode 128, Total Reward: 40.0\n",
            "Episode 129, Total Reward: 103.0\n",
            "Episode 130, Total Reward: 11.0\n",
            "Episode 131, Total Reward: 43.0\n",
            "Episode 132, Total Reward: 40.0\n",
            "Episode 133, Total Reward: 157.0\n",
            "Episode 134, Total Reward: 87.0\n",
            "Episode 135, Total Reward: 34.0\n",
            "Episode 136, Total Reward: 32.0\n",
            "Episode 137, Total Reward: 50.0\n",
            "Episode 138, Total Reward: 41.0\n",
            "Episode 139, Total Reward: 22.0\n",
            "Episode 140, Total Reward: 220.0\n",
            "Environment solved!\n"
          ]
        }
      ],
      "source": [
        "import gymnasium as gym\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "from torch.distributions import Categorical\n",
        "\n",
        "\n",
        "# Function to sample an action based on probabilities\n",
        "def select_action(policy_net, state):\n",
        "    state = torch.FloatTensor(state).unsqueeze(0)  # Add batch dimension\n",
        "    action_probs = policy_net(state)\n",
        "    action_distribution = torch.distributions.Categorical(action_probs)\n",
        "    action = action_distribution.sample()\n",
        "    return action.item(), action_distribution.log_prob(action)\n",
        "\n",
        "\n",
        "# Function to compute discounted returns\n",
        "def compute_returns(rewards, gamma=0.99):\n",
        "    discounted_returns = []\n",
        "    G = 0\n",
        "    for reward in reversed(rewards):\n",
        "        G = reward + gamma * G\n",
        "        discounted_returns.insert(0, G)\n",
        "    return discounted_returns\n",
        "\n",
        "\n",
        "# Training loop for REINFORCE\n",
        "def train_reinforce(env_name=\"CartPole-v1\", episodes=1000, gamma=0.99, lr=1e-3):\n",
        "    # Initialize environment and policy network\n",
        "    env = gym.make(env_name)\n",
        "    state_dim = env.observation_space.shape[0]\n",
        "    action_dim = env.action_space.n\n",
        "    policy_net = PolicyNetwork(state_dim, action_dim) # TODO : Initialize the policy network\n",
        "    optimizer = optim.Adam(policy_net.parameters(), lr=lr)\n",
        "\n",
        "    for episode in range(episodes):\n",
        "        state, _ = env.reset()\n",
        "        log_probs = []\n",
        "        rewards = []\n",
        "\n",
        "        # Collect a single trajectory\n",
        "        done = False\n",
        "        while not done:\n",
        "            action, log_prob = select_action(policy_net, state)  # TODO : Select an action using the policy network\n",
        "            next_state, reward, done, _, _ = env.step(action)\n",
        "            log_probs.append(log_prob)\n",
        "            rewards.append(reward)\n",
        "            state = next_state\n",
        "\n",
        "        # Compute discounted returns\n",
        "        returns = compute_returns(rewards, gamma)\n",
        "        returns = torch.FloatTensor(returns)\n",
        "\n",
        "        # Normalize returns for numerical stability\n",
        "        returns = (returns - returns.mean()) / (returns.std() + 1e-9)\n",
        "\n",
        "        # Compute policy gradient loss\n",
        "        loss = 0\n",
        "        for log_prob, G in zip(log_probs, returns):\n",
        "            loss += -log_prob * G # TODO : Compute the policy gradient loss\n",
        "\n",
        "        # Backpropagation\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Logging\n",
        "        total_reward = sum(rewards)\n",
        "        print(f\"Episode {episode + 1}, Total Reward: {total_reward}\")\n",
        "\n",
        "        # Optional: Stop training early if the task is solved\n",
        "        if total_reward >= 195:  # CartPole-v1 is considered solved at 195\n",
        "            print(\"Environment solved!\")\n",
        "            break\n",
        "\n",
        "    env.close()\n",
        "    return policy_net\n",
        "\n",
        "\n",
        "# Train the policy network\n",
        "trained_policy = train_reinforce()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HJJwfB6CabNn"
      },
      "source": [
        "**Q.6 Test multiple configurations of networks, and multiple environments ( CartPole, CliffWalking, etc). Analyse the training process and your results ;  write a paragraph on the impact of your  architecture choice and on the impact of changing the environment on the training process.**"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 383
        },
        "id": "QkoZwPJfGPen",
        "outputId": "36c41dc4-62b8-4eb4-b167-f757b7cb9b3f"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training softmax on CartPole-v1...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'create_preprocessing_states' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-40-f98c5b949ce3>\u001b[0m in \u001b[0;36m<cell line: 204>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    206\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpolicy\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"softmax\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m              \u001b[0;32mfor\u001b[0m \u001b[0mn_components\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marchitectures\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"softmax\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"n_components\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 208\u001b[0;31m                   \u001b[0mtrain_and_test\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpolicy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mhyperparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_components\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mn_components\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    209\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mpolicy\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"network\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m               \u001b[0;32mfor\u001b[0m \u001b[0mhidden_dims\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marchitectures\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"network\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"hidden_dims\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-40-f98c5b949ce3>\u001b[0m in \u001b[0;36mtrain_and_test\u001b[0;34m(env_name, policy_type, episodes, gamma, lr, hidden_dims, n_components)\u001b[0m\n\u001b[1;32m    162\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Training {policy_type} on {env_name}...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mpolicy_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"softmax\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 164\u001b[0;31m       \u001b[0mtrained_policy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_reinforce_softmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepisodes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_components\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    165\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mpolicy_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"network\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m       \u001b[0mtrained_policy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_reinforce_network\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepisodes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_dims\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-40-f98c5b949ce3>\u001b[0m in \u001b[0;36mtrain_reinforce_softmax\u001b[0;34m(env_name, episodes, gamma, lr, n_components)\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;31m# Initialize environment and parameters of the softmax policy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgym\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m     \u001b[0mpreprocessing_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_preprocessing_states\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mphi\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'create_preprocessing_states' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yCvBVyVnabNn"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s2JWkyolabNo"
      },
      "source": [
        "# Baselines"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sigQ5M7GabNo"
      },
      "source": [
        "As seen in the course, it is possible to reduce the variance of the estimator by using baselines. We will focus on learning baselines."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lH7nmoYkabNo"
      },
      "source": [
        "$$\n",
        "\\nabla_\\theta J(\\theta) = \\mathbb{E}_{\\tau \\sim \\pi_\\theta} \\left[ \\sum_{t=0}^\\infty \\nabla_\\theta \\log \\pi_\\theta(a_t | s_t) \\left( G_t - b(s_t) \\right) \\right],\n",
        "$$\n",
        "with\n",
        "- $ G_t $: The return from time $t$.\n",
        "- $b(s_t)$: A baseline function, often parameterized as  $b(s_t) = V(s_t; \\omega)$, where  $V(s_t,\\omega)$ is a value network learned to estimate the value.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r7nOX7NyabNo"
      },
      "source": [
        "The update rule of the policy network is given by:\n",
        "\n",
        "$$\n",
        "\\theta \\leftarrow \\theta + \\alpha \\sum_{t=0}^T \\nabla_\\theta \\log \\pi_\\theta(a_t | s_t) \\left( R_t - b(s_t) \\right),\n",
        "$$\n",
        "\n",
        "where $T$ is the length of the trajectory.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qaY6DvvHabNo"
      },
      "source": [
        "## Learning a  Baseline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MJThAC1qabNo"
      },
      "source": [
        "The value network $V(s; \\omega)$ is updated to minimize the mean squared error (MSE) between the predicted state value $V(s_t; \\omega)$ and the observed return $G_t$. The objective function for the value network is:\n",
        "\n",
        "$$\n",
        "L(\\omega) = \\mathbb{E}_{\\tau \\sim \\pi_\\theta} \\left[ \\left( G_t - V(s_t; \\omega) \\right)^2 \\right].\n",
        "$$\n",
        "\n",
        "The gradient of the loss function with respect to the value network parameters $\\omega$ is:\n",
        "\n",
        "$$\n",
        "\\nabla_\\omega L(\\omega) = - \\mathbb{E}_{\\tau \\sim \\pi_\\theta} \\left[ \\left( G_t - V(s_t; \\omega) \\right) \\nabla_\\omega V(s_t; \\omega) \\right].\n",
        "$$\n",
        "\n",
        "In practice, this gradient is estimated using sampled trajectories, and the value network parameters are updated using gradient descent:\n",
        "\n",
        "$$\n",
        "\\omega \\leftarrow \\omega - \\beta \\nabla_\\omega L(\\omega),\n",
        "$$\n",
        "\n",
        "where:\n",
        "- $\\omega$: Parameters of the value network.\n",
        "- $\\beta$: Learning rate for the value network.\n",
        "\n",
        "- $V(s_t; \\omega)$: The value network's prediction for the state value at $s_t$.\n",
        "\n",
        "The value network thus learns to approximate the expected return for each state.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LSw54bhSabNp"
      },
      "source": [
        "Let's define the value network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gHSDEZfGabNp"
      },
      "outputs": [],
      "source": [
        "# Define the Value Network\n",
        "class ValueNetwork(nn.Module):\n",
        "    def __init__(self, state_dim, hidden_dim=128):\n",
        "        super(ValueNetwork, self).__init__()\n",
        "        self.fc1 = nn.Linear(state_dim, hidden_dim)\n",
        "        self.fc2 = nn.Linear(hidden_dim, 1)\n",
        "\n",
        "    def forward(self, state):\n",
        "        x = torch.relu(self.fc1(state))\n",
        "        value = self.fc2(x)\n",
        "        return value\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sahimKieabNp"
      },
      "source": [
        "**Q.7 Complete this REINFORCE algorithm with a learned baseline.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u7XiV4ZQabNq"
      },
      "source": [
        "![reinforce_algorithm_baseline](REINFORCE_baseline.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yHxvof1DabNq"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "# Training loop for REINFORCE with baseline\n",
        "def train_reinforce_with_baseline(\n",
        "    env_name=\"CartPole-v1\", episodes=1000, gamma=0.99, lr_policy=1e-3, lr_value=1e-3\n",
        "):\n",
        "    # Initialize environment, policy network, and value network\n",
        "    env = gym.make(env_name)\n",
        "    state_dim = env.observation_space.shape[0]\n",
        "    action_dim = env.action_space.n\n",
        "    policy_net = # TODO : Initialize the policy network\n",
        "    value_net = ValueNetwork(state_dim)\n",
        "\n",
        "    optimizer_policy = optim.Adam(policy_net.parameters(), lr=lr_policy)\n",
        "    optimizer_value = optim.Adam(value_net.parameters(), lr=lr_value)\n",
        "\n",
        "    for episode in range(episodes):\n",
        "        state, _ = env.reset()\n",
        "        log_probs = []\n",
        "        rewards = []\n",
        "        values = []\n",
        "\n",
        "        # Collect a single trajectory\n",
        "        done = False\n",
        "        while not done:\n",
        "            value = value_net(\n",
        "                torch.FloatTensor(state).unsqueeze(0)\n",
        "            )  # Estimate value of current state\n",
        "            action, log_prob = # TODO : Select an action using the policy network\n",
        "            next_state, reward, done, _, _ = env.step(action)\n",
        "\n",
        "            log_probs.append(log_prob)\n",
        "            rewards.append(reward)\n",
        "            values.append(value.squeeze(0))\n",
        "\n",
        "            state = next_state\n",
        "\n",
        "        # Compute discounted returns\n",
        "        returns = compute_returns(rewards, gamma)\n",
        "        returns = torch.FloatTensor(returns)\n",
        "\n",
        "        # Compute baseline-adjusted advantages\n",
        "        values = torch.cat(values)\n",
        "        advantages = (returns - values).detach()\n",
        "\n",
        "        # Update value network\n",
        "        value_loss = # TODO : Compute the value loss (minimize MSE between values and returns)\n",
        "        optimizer_value.zero_grad()\n",
        "        value_loss.backward()\n",
        "        optimizer_value.step()\n",
        "\n",
        "        # Compute policy gradient loss with baseline\n",
        "        policy_loss = 0\n",
        "        for log_prob, advantage in zip(log_probs, advantages):\n",
        "            policy_loss = # TODO : Compute the policy gradient loss with baselines\n",
        "\n",
        "        # Update policy network\n",
        "        optimizer_policy.zero_grad()\n",
        "        policy_loss.backward()\n",
        "        optimizer_policy.step()\n",
        "\n",
        "        # Logging\n",
        "        total_reward = sum(rewards)\n",
        "        print(\n",
        "            f\"Episode {episode + 1}, Total Reward: {total_reward}, Value Loss: {value_loss.item()}\"\n",
        "        )\n",
        "\n",
        "        # Optional: Stop training early if the task is solved\n",
        "        if total_reward >= 195:\n",
        "            print(\"Environment solved!\")\n",
        "            break\n",
        "\n",
        "    env.close()\n",
        "    return policy_net, value_net\n",
        "\n",
        "\n",
        "# Train the policy network with a baseline\n",
        "trained_policy, trained_value = train_reinforce_with_baseline()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NWoMMCAeabNq"
      },
      "source": [
        "**Q.8 Test multiple configurations of value networks, on multiple environments ( CartPole, CliffWalking, etc). Analyse the training process and your results;  write a paragraph comparing baseline usage with no baseline usage and the impact of the choice of architecture.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vAme2bnrabNq"
      },
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "deephypertest",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}